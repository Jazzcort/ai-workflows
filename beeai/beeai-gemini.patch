diff --git a/python/beeai_framework/adapters/gemini/__init__.py b/python/beeai_framework/adapters/gemini/__init__.py
new file mode 100644
index 00000000..23e44d6d
--- /dev/null
+++ b/python/beeai_framework/adapters/gemini/__init__.py
@@ -0,0 +1,7 @@
+# Copyright 2025 © BeeAI a Series of LF Projects, LLC
+# SPDX-License-Identifier: Apache-2.0
+
+from beeai_framework.adapters.gemini.backend.chat import GeminiChatModel
+from beeai_framework.adapters.gemini.backend.embedding import GeminiEmbeddingModel
+
+__all__ = ["GeminiChatModel", "GeminiEmbeddingModel"]
diff --git a/python/beeai_framework/adapters/gemini/backend/__init__.py b/python/beeai_framework/adapters/gemini/backend/__init__.py
new file mode 100644
index 00000000..32f1f133
--- /dev/null
+++ b/python/beeai_framework/adapters/gemini/backend/__init__.py
@@ -0,0 +1,3 @@
+# Copyright 2025 © BeeAI a Series of LF Projects, LLC
+# SPDX-License-Identifier: Apache-2.0
+
diff --git a/python/beeai_framework/adapters/gemini/backend/chat.py b/python/beeai_framework/adapters/gemini/backend/chat.py
new file mode 100644
index 00000000..e33d48ed
--- /dev/null
+++ b/python/beeai_framework/adapters/gemini/backend/chat.py
@@ -0,0 +1,35 @@
+# Copyright 2025 © BeeAI a Series of LF Projects, LLC
+# SPDX-License-Identifier: Apache-2.0
+
+import os
+
+from typing_extensions import Unpack
+
+from beeai_framework.adapters.litellm import utils
+from beeai_framework.adapters.litellm.chat import LiteLLMChatModel
+from beeai_framework.backend.chat import ChatModelKwargs
+from beeai_framework.backend.constants import ProviderName
+from beeai_framework.logger import Logger
+
+logger = Logger(__name__)
+
+
+class GeminiChatModel(LiteLLMChatModel):
+    @property
+    def provider_id(self) -> ProviderName:
+        return "gemini"
+
+    def __init__(
+        self,
+        model_id: str | None = None,
+        **kwargs: Unpack[ChatModelKwargs],
+    ) -> None:
+        super().__init__(
+            model_id if model_id else os.getenv("GEMINI_CHAT_MODEL", "gemini-2.5-flash"),
+            provider_id="gemini",
+            **kwargs,
+        )
+
+        self._settings["extra_headers"] = utils.parse_extra_headers(
+            self._settings.get("extra_headers"), os.getenv("GEMINI_API_HEADERS")
+        )
diff --git a/python/beeai_framework/adapters/gemini/backend/embedding.py b/python/beeai_framework/adapters/gemini/backend/embedding.py
new file mode 100644
index 00000000..23059b6c
--- /dev/null
+++ b/python/beeai_framework/adapters/gemini/backend/embedding.py
@@ -0,0 +1,32 @@
+# Copyright 2025 © BeeAI a Series of LF Projects, LLC
+# SPDX-License-Identifier: Apache-2.0
+
+import os
+
+from typing_extensions import Unpack
+
+from beeai_framework.adapters.litellm import utils
+from beeai_framework.adapters.litellm.embedding import LiteLLMEmbeddingModel
+from beeai_framework.backend.constants import ProviderName
+from beeai_framework.backend.embedding import EmbeddingModelKwargs
+
+
+class GeminiEmbeddingModel(LiteLLMEmbeddingModel):
+    @property
+    def provider_id(self) -> ProviderName:
+        return "gemini"
+
+    def __init__(
+        self,
+        model_id: str | None = None,
+        **kwargs: Unpack[EmbeddingModelKwargs],
+    ) -> None:
+        super().__init__(
+            model_id if model_id else os.getenv("GEMINI_EMBEDDING_MODEL", "gemini-embedding-001"),
+            provider_id="gemini",
+            **kwargs,
+        )
+
+        self._settings["extra_headers"] = utils.parse_extra_headers(
+            self._settings.get("extra_headers"), os.getenv("GEMINI_API_HEADERS")
+        )
diff --git a/python/beeai_framework/backend/constants.py b/python/beeai_framework/backend/constants.py
index a0270d22..65ede1f5 100644
--- a/python/beeai_framework/backend/constants.py
+++ b/python/beeai_framework/backend/constants.py
@@ -6,10 +6,10 @@ from typing import Literal
 from pydantic import BaseModel
 
 ProviderName = Literal[
-    "ollama", "openai", "watsonx", "groq", "xai", "vertexai", "amazon_bedrock", "anthropic", "azure_openai", "mistralai"
+    "ollama", "openai", "watsonx", "groq", "xai", "vertexai", "gemini", "amazon_bedrock", "anthropic", "azure_openai", "mistralai"
 ]
 ProviderHumanName = Literal[
-    "Ollama", "OpenAI", "Watsonx", "Groq", "XAI", "VertexAI", "AmazonBedrock", "Anthropic", "AzureOpenAI", "MistralAI"
+    "Ollama", "OpenAI", "Watsonx", "Groq", "XAI", "VertexAI", "Gemini", "AmazonBedrock", "Anthropic", "AzureOpenAI", "MistralAI"
 ]
 
 
@@ -32,6 +32,7 @@ BackendProviders = {
     "Groq": ProviderDef(name="Groq", module="groq", aliases=["groq"]),
     "xAI": ProviderDef(name="XAI", module="xai", aliases=["xai", "grok"]),
     "vertexAI": ProviderDef(name="VertexAI", module="vertexai", aliases=["vertexai", "google"]),
+    "Gemini": ProviderDef(name="Gemini", module="gemini", aliases=["gemini"]),
     "AmazonBedrock": ProviderDef(
         name="AmazonBedrock",
         module="amazon_bedrock",
